{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "팀플-bilstm",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZUcLgzqth2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###출처: https://github.com/2alive3s/Fake_news\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#from text_cnn import TextCNN\n",
        "from tensorflow.contrib.rnn import GRUCell\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn #brnn layer를 위해\n",
        "#from tensorflow.contrib.rnn import stack_bidirectional_rnn as bi_rnn\n",
        "\n",
        "\n",
        "class Affine(object):\n",
        "# Combine all the pooled features\n",
        "    def __init__( #초기화 메서드(새로운 인스턴스 만들어질때 호출)\n",
        "      self, sequence_length_head, sequence_length_body, num_classes, vocab_size_head, vocab_size_body,\n",
        "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.1): #embedding_size:embedding vector크기\n",
        "        \n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")     #예측출력갯수\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")      #dropout\n",
        "        self.input_x_head = tf.placeholder(tf.int32, [None, sequence_length_head], name=\"input_x_head\")  #input x의 헤더\n",
        "        self.input_x_body = tf.placeholder(tf.int32, [None, sequence_length_body], name=\"input_x_body\")  #input x의 바디\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embeddings_head = tf.Variable(\n",
        "                tf.random_uniform([vocab_size_head, embedding_size], -1.0, 1.0),trainable=False)#trainable=false #모델 결과 값인 임베딩 벡터를 저장할 변수\n",
        "                                                                                                #총 단어 갯수와 임베딩 갯수를 크기로 하는 두개의 차원을 가짐 \n",
        "        self.embedded_chars_head = tf.nn.embedding_lookup(self.embeddings_head, self.input_x_head) #큰사이즈의 리스트에 담겨진 데이터를 입력받은 인덱스에 따라 look up\n",
        "                                                                                                   #embeddings_head와 input_x_head가 처리할 인덱스\n",
        "                                                                                                   # 임베딩 벡터의 차원에서 학습할 입력값에 대한 행들을 뽑음\n",
        "                                                                                                   # 예) embeddings     inputs    selected\n",
        "                                                                                                   #    [[1, 2, 3]  -> [2, 3] -> [[2, 3, 4]\n",
        "                                                                                                   #     [2, 3, 4]                [3, 4, 5]]\n",
        "                                                                                                   #     [3, 4, 5]\n",
        "                                                                                                   #     [4, 5, 6]]\n",
        "        self.embedded_chars_expanded_head = tf.expand_dims(self.embedded_chars_head, -1)\n",
        "\n",
        "        self.embeddings_body = tf.Variable(\n",
        "                tf.random_uniform([vocab_size_body, embedding_size], -1.0, 1.0),trainable=False)#trainable=false\n",
        "        self.embedded_chars_body = tf.nn.embedding_lookup(self.embeddings_body, self.input_x_body)\n",
        "        self.embedded_chars_expanded_body = tf.expand_dims(self.embedded_chars_body, -1) #expand_dims: 크기가 1인차원을 텐서의 구조에 삽입\n",
        "                                                                                         #음수가 지정되었으므로, 끝에서부터 역으로 계산\n",
        "        \n",
        "        #2. LSTM LAYER ######################################################################\n",
        "        with tf.variable_scope(\"lstm-head\") as scope: #lstm-head/변수이름 // 변수이름충돌방지\n",
        "            #self.lstm_cell_head = tf.contrib.rnn.LSTMCell(embedding_size,state_is_tuple=True)\n",
        "            #self.lstm_out_head,self.lstm_state_head = tf.nn.dynamic_rnn(self.lstm_cell_head,self.embedded_chars_head,dtype=tf.float32)\n",
        "            #self.lstm_out_expanded_head = tf.expand_dims(self.lstm_out_head, -1)\n",
        "            self.lstm_out_head,self.lstm_state_head = bi_rnn(GRUCell(embedding_size), GRUCell(embedding_size), inputs = self.embedded_chars_head , dtype=tf.float32) #gru셀 계산\n",
        "            self.lstm_out_merge_head = tf.concat(self.lstm_out_head, axis=2) #lst_out_head를 2번째차원을 제거후 붙이겠다\n",
        "            #self.lstm_out_head_fw = self.lstm_out_head[0]\n",
        "            #self.lstm_out_head_bw = self.lstm_out_head[1]\n",
        "            #self.lstm_out_merge_head = tf.concat([self.lstm_out_head_fw[-1], self.lstm_out_head_bw[-1]], axis=1)\n",
        "            self.lstm_out_expanded_head = tf.expand_dims(self.lstm_out_merge_head, -1) #크기가 1인차원 끝에 삽입\n",
        "            print(self.lstm_out_expanded_head.shape)\n",
        "\n",
        "#output = tf.stack(output, axis=1)\n",
        "#output = tf.reshape(output, [-1, FLAGS.num_units * 2])\n",
        "\n",
        "        with tf.variable_scope(\"lstm-body\") as scope:\n",
        "            #self.lstm_cell_body = tf.contrib.rnn.LSTMCell(embedding_size,state_is_tuple=True)\n",
        "            #self.lstm_out_body,self.lstm_state_body = tf.nn.dynamic_rnn(self.lstm_cell_body,self.embedded_chars_body,dtype=tf.float32)\n",
        "            #self.lstm_out_expanded_body = tf.expand_dims(self.lstm_out_body, -1)\n",
        "            self.lstm_out_body,self.lstm_state_body = bi_rnn(GRUCell(embedding_size), GRUCell(embedding_size), inputs = self.embedded_chars_body , dtype=tf.float32)\n",
        "            self.lstm_out_merge_body = tf.concat(self.lstm_out_body, axis=2)\n",
        "            #self.lstm_out_body_fw = self.lstm_out_body[0]\n",
        "            #self.lstm_out_body_bw = self.lstm_out_body[1]\n",
        "            #self.lstm_out_merge_body = tf.concat([self.lstm_out_body_fw[-1], self.lstm_out_body_bw[-1]], axis=1)\n",
        "            self.lstm_out_expanded_body = tf.expand_dims(self.lstm_out_merge_body, -1)\n",
        "            \n",
        "            print(self.lstm_out_expanded_body.shape)\n",
        "        \n",
        "        self.pooled_outputs_head = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-head-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, embedding_size*2, 1, 256] #256의 컨볼루션 값=> 256으로 늘리겠다\n",
        "                W_head = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W_head\")\n",
        "                b_head = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b_head\")\n",
        "                conv_head = tf.nn.conv2d(\n",
        "                    self.lstm_out_expanded_head,\n",
        "                    W_head,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h_head = tf.nn.relu(tf.nn.bias_add(conv_head, b_head), name=\"relu_head\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled_head = tf.nn.max_pool(\n",
        "                    h_head,\n",
        "                    ksize=[1, sequence_length_head - filter_size + 1, 1, 1], #커널사이즈 sequence_length_head-filter_size+1,1 #왜 이렇게 바꾸는지 알아보기!??\n",
        "                    strides=[1, 1, 1, 1],                                    #max_pooling 적용후 텐서의 차원 수 [batch_size,1,1num_filters]\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                self.pooled_outputs_head.append(pooled_head) #pooled_ouputs_head의 풀링거친 첫번째 컨볼루션레이어값 추가\n",
        "\n",
        "        self.pooled_outputs_body = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-body-%s\" % filter_size):\n",
        "                # Convolution Layer\n",
        "                filter_shape = [filter_size, embedding_size*2, 1, 1024] \n",
        "                W_body = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W_body\")\n",
        "                b_body = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b_body\")\n",
        "                conv_body = tf.nn.conv2d(\n",
        "                    self.lstm_out_expanded_body,\n",
        "                    W_body,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h_body = tf.nn.relu(tf.nn.bias_add(conv_body, b_body), name=\"relu_body\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled_body = tf.nn.max_pool(\n",
        "                    h_body,\n",
        "                    ksize=[1, sequence_length_body - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                self.pooled_outputs_body.append(pooled_body)        \n",
        "        \n",
        "        l2_loss = tf.constant(0.0) #0으로 설정\n",
        "        \n",
        "        pooled_outputs = tf.concat([self.pooled_outputs_head,self.pooled_outputs_body],-1,name='preconcat') #value(-1)차원으로 이어 붙이기\n",
        "        print(pooled_outputs.shape)\n",
        "        num_filters_total = num_filters * len(filter_sizes) #토탈필터사이즈구하기\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3, name='concat')  #pooled_outputs을 3번째차원을 제거후 붙이겠다\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total]) #fully connected layer로 넣기 위해 펼치기\n",
        "       \n",
        "       \t######fully connected layer\n",
        "        W_fc1 = tf.Variable(tf.truncated_normal([1280,1024],stddev=0.1),name=\"W_fc1\") \n",
        "        b_fc1 = tf.Variable(tf.constant(0.1,shape=[1024]),name=\"b_fc1\")\n",
        "        h_fc1 = tf.nn.relu(tf.matmul(self.h_pool_flat,W_fc1) + b_fc1) #relu사용\n",
        "        \n",
        "        W_fc2 = tf.Variable(tf.truncated_normal([1024,1024],stddev=0.1),name=\"W_fc1\")\n",
        "        b_fc2 = tf.Variable(tf.constant(0.1,shape=[1024]),name=\"b_fc1\")\n",
        "        h_fc2 = tf.nn.relu(tf.matmul(h_fc1,W_fc2) + b_fc2)\n",
        "        \n",
        "        W_fc3 = tf.Variable(tf.truncated_normal([1024,1024],stddev=0.1),name=\"W_fc1\")\n",
        "        b_fc3 = tf.Variable(tf.constant(0.1,shape=[1024]),name=\"b_fc1\") #ouput 전 1024\n",
        "        h_fc3 = tf.nn.relu(tf.matmul(h_fc2,W_fc3) + b_fc3)\n",
        "            \n",
        "        # Add dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(h_fc3, self.dropout_keep_prob) #마지막 h_fc3만 dropout?//그럼 중간중간 droupout 추가하면 정확도증가???(시도!)\n",
        "\n",
        "        # Final (unnormalized) scores and predictions\n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[1024, num_classes],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\") #마지막 output shape로 num_classes까지 줄이기\n",
        "            l2_loss += tf.nn.l2_loss(W) #tf.nn.l2_loss는 전달받은 텐서의 요소들의 제곱의 합을 2로 나누어 리턴\n",
        "            l2_loss += tf.nn.l2_loss(b) #w와 b를ㅣ2_loss로 리턴한후 그값들을 l2_loss변수에 추가\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, self.W, self.b, name=\"scores\") #Computes matmul(x, weights) + biases\n",
        "            self.probabilities = tf.nn.softmax(self.scores)\n",
        "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "        # CalculateMean cross-entropy loss\n",
        "        with tf.name_scope(\"loss\"): #손실함수 정의\n",
        "            print(self.scores.shape)\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(logits = self.scores, labels = self.input_y)\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            print(\"%d/%d\",self.predictions,self.input_y)\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYmMG5pqGgfw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2340
        },
        "outputId": "1386345a-1f62-4230-d4ab-97da2542ff9a"
      },
      "source": [
        "# -*- coding: utf-8 -*-:\n",
        "\"\"\"\n",
        "Created on Wed Nov  8 23:58:05 2017\n",
        "@author: samsung\n",
        "\"\"\"\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk\n",
        "!pip3 install konlpy\n",
        "\n",
        "import numpy as np\n",
        "import codecs\n",
        "import re\n",
        "import itertools\n",
        "from collections import Counter\n",
        "from konlpy.tag import Komoran\n",
        "from csv import DictReader\n",
        "from csv import DictWriter\n",
        "\n",
        "pos_tagger = Komoran()\n",
        "\n",
        "class Data:\n",
        "    def __init__(self, file_instances):\n",
        "        # Load data\n",
        "        self.instances = self.read(file_instances)\n",
        "        self.headlines = {}\n",
        "        self.bodies = {}\n",
        "        self.labels = {}\n",
        "         \n",
        "        for instance in self.instances:\n",
        "            instance['seqid'] = int(instance['seqid'])\n",
        "            \n",
        "        for head in self.instances:\n",
        "            self.headlines[head['seqid']] = head['title']\n",
        "        \n",
        "        for body in self.instances:\n",
        "            self.bodies[body['seqid']] = body['content']\n",
        "        \n",
        "        for label in self.instances:\n",
        "            self.labels[label['seqid']] = label['Label']\n",
        "                \n",
        "    def read(self, filename):\n",
        "        \n",
        "        rows = []\n",
        "        # Process file\n",
        "        with open(filename, \"r\") as table:\n",
        "            r = DictReader(table)\n",
        "            for line in r:\n",
        "                rows.append(line)\n",
        "        return rows\n",
        "    \n",
        "    def get_data(self):\n",
        "        return self.instances\n",
        "        \n",
        "def tokenize(string):\n",
        "    \"\"\"\n",
        "    Tokenization/string cleaning for all datasets except for SST.\n",
        "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
        "    \"\"\"\n",
        "    final_string = ''\n",
        "    tokenized = [''.join(t) for t in pos_tagger.pos(string) if t[1] in ['NNG','NNP','NNB','NR','VV','VA','VCP','VCN','XSV','XSA','SN','MAG','MM','MAJ']]\n",
        "#not in ['ETM','EC','ETN','EF','EP','NF','NV','NA','SW','SO','SP','SF','SE','SS','IC','XSN','XPN']]\n",
        "    for tokens in tokenized:\n",
        "        final_string += tokens + \" \"\n",
        "    return final_string\n",
        "\n",
        "def flat(content):\n",
        "    return [\"{}{}\".format(word, tag) for word, tag in pos_tagger.pos(content)]\n",
        "\n",
        "\n",
        "def load_data_and_labels(file_instances):\n",
        "    \"\"\"\n",
        "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
        "    Returns split sentences and labels.\n",
        "    \"\"\"\n",
        "    # Load data from files\n",
        "\n",
        "    train_heads = []\n",
        "    train_bodies = []\n",
        "    train_labels = []\n",
        "    data = Data(file_instances)\n",
        "    \n",
        "    for instance in data.instances:\n",
        "        news_id = instance['seqid']\n",
        "        train_label = instance['Label']\n",
        "        train_heads.append(tokenize(data.headlines[news_id]))\n",
        "        train_bodies.append(tokenize(data.bodies[news_id]))\n",
        "        train_labels.append((news_id,train_label))\n",
        "\n",
        "    results = np.zeros((len(train_labels),2))\n",
        "\n",
        "    for i, train_label in train_labels:\n",
        "        if train_label == '1':\n",
        "            results[i,1] = 1\n",
        "        else:\n",
        "            results[i,0] = 1\n",
        "\n",
        "    return train_heads, train_bodies, results\n",
        "\n",
        "\n",
        "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
        "    \"\"\"\n",
        "    Generates a batch iterator for a dataset.\n",
        "    \"\"\"\n",
        "    data = np.array(data)\n",
        "    data_size = len(data)\n",
        "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
        "    for epoch in range(num_epochs):\n",
        "        # Shuffle the data at each epoch\n",
        "        if shuffle:\n",
        "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
        "            shuffled_data = data[shuffle_indices]\n",
        "        else:\n",
        "            shuffled_data = data\n",
        "        for batch_num in range(num_batches_per_epoch):\n",
        "            start_index = batch_num * batch_size\n",
        "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
        "            yield shuffled_data[start_index:end_index]\n",
        "\n",
        "def load_word_embedding(file_name, vocab_processor, embedding_dim):\n",
        "\n",
        "    initW = np.random.uniform(-0.25, 0.25, (len(vocab_processor.vocabulary_), embedding_dim))\n",
        "    print(\"Load word2vec file {}\\n\".format(file_name))\n",
        "    with open(file_name, \"rb\") as f:\n",
        "        for idx, line in enumerate(f):\n",
        "            word = []\n",
        "            vectors = []\n",
        "            if idx == 0:\n",
        "                vocab_size, dim = line.strip().split()\n",
        "            else:\n",
        "                tks = line.strip().split()\n",
        "                word = tks[0].strip().decode('utf8')\n",
        "                vectors = tks[1:]\n",
        "                idx = vocab_processor.vocabulary_.get(word)\n",
        "                if idx != 0:\n",
        "                    initW[idx] = np.array(vectors)\n",
        "    print(initW)\n",
        "    return initW"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.149)] [1 InRelease 14.2 kB/88.7\r                                                                               \rHit:2 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [801 B]\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [320 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:13 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,609 B]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [463 kB]\n",
            "Get:16 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,643 kB]\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [85.5 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [808 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,210 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [2,496 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [3,906 B]\n",
            "Get:22 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [788 kB]\n",
            "Fetched 5,596 kB in 6s (865 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:7.4.0-1ubuntu2.2).\n",
            "g++ set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libxxf86dga1 openjdk-8-jre x11-utils\n",
            "Suggested packages:\n",
            "  gvfs openjdk-8-demo openjdk-8-source visualvm icedtea-8-plugin mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  fonts-dejavu-core fonts-dejavu-extra libatk-wrapper-java\n",
            "  libatk-wrapper-java-jni libgail-common libgail18 libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libxxf86dga1 openjdk-8-jdk openjdk-8-jre x11-utils\n",
            "0 upgraded, 13 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 6,847 kB of archives.\n",
            "After this operation, 19.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-core all 2.37-1 [1,041 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 fonts-dejavu-extra all 2.37-1 [1,953 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java all 0.33.3-20ubuntu0.1 [34.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk-wrapper-java-jni amd64 0.33.3-20ubuntu0.1 [28.3 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-common all 2.24.32-1ubuntu1 [125 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-0 amd64 2.24.32-1ubuntu1 [1,769 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail18 amd64 2.24.32-1ubuntu1 [14.2 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail-common amd64 2.24.32-1ubuntu1 [112 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-bin amd64 2.24.32-1ubuntu1 [7,536 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jre amd64 8u212-b03-0ubuntu1.18.04.1 [69.4 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 openjdk-8-jdk amd64 8u212-b03-0ubuntu1.18.04.1 [1,483 kB]\n",
            "Fetched 6,847 kB in 1s (6,668 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 130911 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../01-fonts-dejavu-core_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../02-fonts-dejavu-extra_2.37-1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../03-x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../04-libatk-wrapper-java_0.33.3-20ubuntu0.1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../05-libatk-wrapper-java-jni_0.33.3-20ubuntu0.1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../06-libgtk2.0-common_2.24.32-1ubuntu1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../07-libgtk2.0-0_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../08-libgail18_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../09-libgail-common_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../10-libgtk2.0-bin_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package openjdk-8-jre:amd64.\n",
            "Preparing to unpack .../11-openjdk-8-jre_8u212-b03-0ubuntu1.18.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jre:amd64 (8u212-b03-0ubuntu1.18.04.1) ...\n",
            "Selecting previously unselected package openjdk-8-jdk:amd64.\n",
            "Preparing to unpack .../12-openjdk-8-jdk_8u212-b03-0ubuntu1.18.04.1_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk:amd64 (8u212-b03-0ubuntu1.18.04.1) ...\n",
            "Setting up libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Setting up fonts-dejavu-core (2.37-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Setting up libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libatk-wrapper-java (0.33.3-20ubuntu0.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.33.3-20ubuntu0.1) ...\n",
            "Setting up openjdk-8-jre:amd64 (8u212-b03-0ubuntu1.18.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/policytool to provide /usr/bin/policytool (policytool) in auto mode\n",
            "Setting up openjdk-8-jdk:amd64 (8u212-b03-0ubuntu1.18.04.1) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/3d/4e983cd98d87b50b2ab0387d73fa946f745aa8164e8888a714d5129f9765/konlpy-0.5.1-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 2.7MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.5.7 (from konlpy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/4b/60a3e63d51714d4d7ef1b1efdf84315d118a0a80a5b085bb52a7e2428cdc/JPype1-0.6.3.tar.gz (168kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 48.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: JPype1\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/2b/e8/c0b818ac4b3d35104d35e48cdc7afe27fc06ea277feed2831a\n",
            "Successfully built JPype1\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-0.6.3 konlpy-0.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AQgl2zIv104",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "313eaa56-f9ee-4ac1-fdf4-3da46d15795d"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/gdrive')==False:\n",
        "  drive.mount('/content/gdrive')\n",
        "  print('Google Drive is mounted\\n')\n",
        "else:\n",
        "  print('Google Drive is already mounted\\n')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Google Drive is mounted\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcFv3HOdIzCo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "0b3e6bda-1db8-400f-f2d9-f8fa9b8866a8"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Nov  8 23:58:50 2017\n",
        "@author: samsung\n",
        "\"\"\"\n",
        "\n",
        "#! /usr/bin/env python\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "#import data_helpers\n",
        "#from affine import Affine\n",
        "from tensorflow.contrib import learn\n",
        "import codecs\n",
        "import operator\n",
        "from pandas.io.parsers import read_csv\n",
        "\n",
        "'''\n",
        "# Parameters\n",
        "# ==================================================\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "# Model Hyperparameters\n",
        "tf.flags.DEFINE_integer(\"embedding_dim\",128, \"Dimensionality of character embedding (default: 128)\")\n",
        "tf.flags.DEFINE_string(\"filter_sizes\", \"3\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
        "tf.flags.DEFINE_integer(\"num_filters\", 1280, \"Number of filters per filter size (default: 128)\")\n",
        "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
        "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularizaion lambda (default: 0.0)\")\n",
        "\n",
        "# Training parameters\n",
        "tf.flags.DEFINE_integer(\"batch_size\",127, \"Batch Size (default: 64)\")\n",
        "tf.flags.DEFINE_integer(\"num_epochs\", 100, \"Number of training epochs (default: 200)\")\n",
        "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
        "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
        "# Misc Parameters\n",
        "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "FLAGS._parse_flags()\n",
        "print(\"\\nParameters:\")\n",
        "for attr, value in sorted(FLAGS.__flags.items()):\n",
        "    print(\"{}={}\".format(attr.upper(), value))\n",
        "print(\"\")\n",
        "'''\n",
        "embedding_dim=128\n",
        "filter_sizes=\"3\"\n",
        "num_filters=1280\n",
        "dropout_keep_prob=0.5\n",
        "l2_reg_lambda=0.1 \n",
        "batch_size=127,\n",
        "num_epochs=100\n",
        "evaluate_every=100\n",
        "checkpoint_every=100\n",
        "allow_soft_placement= True\n",
        "log_device_placement= False\n",
        "\n",
        "\n",
        "# Data Preparatopn\n",
        "# ==================================================\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = tf.global_variables_initializer()\n",
        "\n",
        "if os.path.exists('/content/gdrive/My Drive/Colab Notebooks/mission1_test.csv')==False:\n",
        "  print('Train data downloading..')\n",
        "  ! curl 'https://raw.githubusercontent.com/2alive3s/Fake_news/master/data/mission1_test.csv' -o '/content/gdrive/My Drive/Colab Notebooks/mission1_test.csv'\n",
        "  print('Done..\\n')\n",
        "else:\n",
        "  print('File already exists \\n')\n",
        "    \n",
        "# 학습데이터 로딩\n",
        "file_train_instances = read_csv('/content/gdrive/My Drive/Colab Notebooks/mission1_test.csv')\n",
        "\n",
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "x_heads, x_bodies, y = data_helpers.load_data_and_labels(file_train_instances)\n",
        "\n",
        "#bow_head = TfidfVectorizer(tokenizer=None, lowercase=False, max_features=128)\n",
        "#bow_body = TfidfVectorizer(tokenizer=None, lowercase=False, max_features=1280)\n",
        "\n",
        "#bow_head_vec = bow_head.fit_transform(x_heads)\n",
        "#bow_body_vec = bow_body.fit_transform(x_bodies)\n",
        "\n",
        "#bow_head_vocab = [v[0] for v in sorted(bow_head.vocabulary_.items(),key=operator.itemgetter(1))]\n",
        "#print(x_heads)\n",
        "#print(bow_head_vocab)\n",
        "\n",
        "# Build vocabulary_head\n",
        "max_document_length_head = max([len(x.split(\" \")) for x in x_heads])\n",
        "vocab_processor_head = learn.preprocessing.VocabularyProcessor(max_document_length=128)#bow_head max_features\n",
        "x_head = np.array(list(vocab_processor_head.fit_transform(x_heads)))\n",
        "# Build vocabulary_body\n",
        "max_document_length_body = max([len(x.split(\" \")) for x in x_bodies])\n",
        "vocab_processor_body = learn.preprocessing.VocabularyProcessor(max_document_length=1280)\n",
        "x_body = np.array(list(vocab_processor_body.fit_transform(x_bodies)))\n",
        "\n",
        "print('----headline_shape----')\n",
        "print(x_head.shape)\n",
        "print('----body_shape----')\n",
        "print(x_body.shape)\n",
        "print('----label_shape----')\n",
        "print(y.shape)\n",
        "\n",
        "# Randomly shuffle data\n",
        "np.random.seed(10)\n",
        "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
        "\n",
        "x_head_shuffled = x_head[shuffle_indices]\n",
        "x_body_shuffled = x_body[shuffle_indices]\n",
        "y_shuffled = y[shuffle_indices]\n",
        "\n",
        "# Split train/test set\n",
        "# TODO: This is very crude, should use cross-validation\n",
        "x_train_head, x_dev_head = x_head_shuffled[:-6000], x_head_shuffled[-6000:]\n",
        "x_train_body, x_dev_body = x_body_shuffled[:-6000], x_body_shuffled[-6000:]\n",
        "y_train, y_dev = y_shuffled[:-6000], y_shuffled[-6000:]\n",
        "print(\"Vocabulary Size_head: {:d}\".format(len(vocab_processor_head.vocabulary_)))\n",
        "print(\"Vocabulary Size_body: {:d}\".format(len(vocab_processor_body.vocabulary_)))\n",
        "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
        "print(x_train_head)\n",
        "print(x_train_body)\n",
        "print(x_train_head.shape)\n",
        "print(x_train_body.shape)\n",
        "# Training\n",
        "# ==================================================\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    session_conf = tf.ConfigProto(\n",
        "      allow_soft_placement=allow_soft_placement,\n",
        "      log_device_placement=log_device_placement)\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    with sess.as_default():\n",
        "        cnn = Affine(\n",
        "            sequence_length_head=128,\n",
        "            sequence_length_body=1280,\n",
        "            num_classes=2,\n",
        "            vocab_size_head=len(vocab_processor_head.vocabulary_),\n",
        "            vocab_size_body=len(vocab_processor_body.vocabulary_),\n",
        "            embedding_size=embedding_dim,\n",
        "            filter_sizes=list(map(int,filter_sizes.split(\",\"))),\n",
        "            num_filters=num_filters,\n",
        "            l2_reg_lambda=l2_reg_lambda\n",
        "            )\n",
        "\n",
        "        # Define Training procedure\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
        "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        # Keep track of gradient values and sparsity (optional)\n",
        "        grad_summaries = []\n",
        "        for g, v in grads_and_vars:\n",
        "            if g is not None:\n",
        "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
        "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                grad_summaries.append(grad_hist_summary)\n",
        "                grad_summaries.append(sparsity_summary)\n",
        "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "        # Output directory for models and summaries\n",
        "        timestamp = str(int(time.time()))\n",
        "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "        print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Summaries for loss and accuracy\n",
        "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
        "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
        "\n",
        "        # Train Summaries\n",
        "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
        "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "        # Dev summaries\n",
        "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
        "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        saver = tf.train.Saver(tf.global_variables(),max_to_keep=500)\n",
        "\n",
        "        # Write vocabulary\n",
        "        vocab_processor_head.save(os.path.join(out_dir, \"vocab_head\"))\n",
        "        vocab_processor_body.save(os.path.join(out_dir, \"vocab_body\"))\n",
        "        \n",
        "        # Initialize all variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        head_embedding = data_helpers.load_word_embedding(\"fasttext_3_10.vec\",vocab_processor_head,embedding_dim)\n",
        "        body_embedding = data_helpers.load_word_embedding(\"fasttext_3_10.vec\",vocab_processor_body,embedding_dim)\n",
        "        sess.run(cnn.embeddings_head.assign(head_embedding))\n",
        "        sess.run(cnn.embeddings_body.assign(body_embedding))\n",
        " #       sess.run(cnn.cnn_head.W.assign(initW))\n",
        " #       sess.run(cnn.cnn_body.W.assign(initW))\n",
        "            \n",
        "        def train_step(x_batch_head, x_batch_body, y_batch):\n",
        "            \"\"\"\n",
        "            A single training step\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "              cnn.input_x_head: x_batch_head,\n",
        "              cnn.input_x_body: x_batch_body,\n",
        "              cnn.input_y: y_batch,\n",
        "              cnn.dropout_keep_prob: dropout_keep_prob\n",
        "            }\n",
        "            _, step, loss, accuracy, predictions = sess.run(\n",
        "                [train_op, global_step, cnn.loss, cnn.accuracy, cnn.predictions],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            print(predictions)\n",
        "            #train_summary_writer.add_summary(summaries, step)\n",
        "\n",
        "        def dev_step(x_batch_head, x_batch_body, y_batch, writer=None):\n",
        "            \"\"\"\n",
        "            Evaluates model on a dev set\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "              cnn.input_x_head: x_batch_head,\n",
        "              cnn.input_x_body: x_batch_body,\n",
        "              cnn.input_y: y_batch,\n",
        "              cnn.dropout_keep_prob: 1.0\n",
        "            }\n",
        "            step, loss, accuracy, predictions = sess.run(\n",
        "                [global_step, cnn.loss, cnn.accuracy, cnn.predictions],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            return accuracy, loss, predictions\n",
        "            #if writer:\n",
        "            #    writer.add_summary(summaries, step)\n",
        "\n",
        "        # Generate batches\n",
        "        batches = data_helpers.batch_iter(\n",
        "            list(zip(x_train_head, x_train_body, y_train)), batch_size, num_epochs)\n",
        "      \n",
        "        # Training loop. For each batch...\n",
        "        for batch in batches:\n",
        "            x_batch_head, x_batch_body, y_batch = zip(*batch)\n",
        "            train_step(x_batch_head, x_batch_body, y_batch)\n",
        "            current_step = tf.train.global_step(sess, global_step)\n",
        "            if current_step % evaluate_every == 0:\n",
        "                print(\"\\nEvaluation:\")\n",
        "                batches_dev = data_helpers.batch_iter(list(zip(x_dev_head, x_dev_body, y_dev)),31, 1)\n",
        "                acc_total, loss_total, total = 0,0,0\n",
        "                for batch_dev in batches_dev:\n",
        "                    x_batch_dev_head, x_batch_dev_body, y_batch_dev = zip(*batch_dev)\n",
        "                    acc_dev, loss_dev, predictions = dev_step(x_batch_dev_head, x_batch_dev_body, y_batch_dev)\n",
        "                    acc_total = acc_total + acc_dev\n",
        "                    loss_total = loss_total + loss_dev\n",
        "                    print(predictions)\n",
        "                    total = total + 1\n",
        "                results = codecs.open('results_all_new.txt','a')\n",
        "                result = \"total loss {:g}, total acc {:g}\".format(loss_total/total, acc_total/total)\n",
        "                print(result)\n",
        "                results.write(\"step{:g}, total loss{:g}, total acc{:g}\".format(current_step, loss_total/total, acc_total/total) + '\\n')\n",
        "                results.close()\n",
        "                print(\"\")\n",
        "            if current_step % checkpoint_every == 0:\n",
        "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                print(path)\n",
        "                print(\"Saved model checkpoint to {}\\n\".format(path))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File already exists \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fdb47c64366d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# 학습데이터 로딩\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mfile_train_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Colab Notebooks/mission1_test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 32, saw 13\n"
          ]
        }
      ]
    }
  ]
}