{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled29.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XYlZPAi491X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##출처 https://github.com/2alive3s/Fake_news\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sun Nov 12 11:21:21 2017\n",
        "@author: samsung\n",
        "\"\"\"\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from text_cnn import TextCNN\n",
        "from tensorflow.contrib.rnn import GRUCell\n",
        "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
        "#from tensorflow.contrib.rnn import stack_bidirectional_rnn as bi_rnn\n",
        "\n",
        "\n",
        "class Affine(object):\n",
        "# Combine all the pooled features\n",
        "    def __init__(#초기화\n",
        "      self, sequence_length_head, sequence_length_body, num_classes, vocab_size_head, vocab_size_body,\n",
        "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.1):\n",
        "        \n",
        "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
        "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
        "        self.input_x_head = tf.placeholder(tf.int32, [None, sequence_length_head], name=\"input_x_head\")\n",
        "        self.input_x_body = tf.placeholder(tf.int32, [None, sequence_length_body], name=\"input_x_body\")\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embeddings_head = tf.Variable(\n",
        "                tf.random_uniform([vocab_size_head, embedding_size], -1.0, 1.0),trainable=False)#trainable=false\n",
        "        #random_uniform : 균일분포로부터 랜덤값을 출력\n",
        "        self.embedded_chars_head = tf.nn.embedding_lookup(self.embeddings_head, self.input_x_head)\n",
        "        #embedding_lookup : 큰 사이즈의 리스트 형태에 담겨진 데이터를 입력받은 인덱스에 따라 look up해서 사용할 때 이용\n",
        "        self.embedded_chars_expanded_head = tf.expand_dims(self.embedded_chars_head, -1)\n",
        "        #expand_dims : 차원 추가, 확장\n",
        "        self.embeddings_body = tf.Variable(\n",
        "                tf.random_uniform([vocab_size_body, embedding_size], -1.0, 1.0),trainable=False)#trainable=false\n",
        "        self.embedded_chars_body = tf.nn.embedding_lookup(self.embeddings_body, self.input_x_body)\n",
        "        self.embedded_chars_expanded_body = tf.expand_dims(self.embedded_chars_body, -1)\n",
        "        \n",
        "        #word embedding model(단어 임베딩 모델) : 단어를 벡터로 바꿈\n",
        "        #word2vec이 대표적/본 연구에서는 Fasttext 사용\n",
        "        \n",
        "        #2. LSTM LAYER ######################################################################\n",
        "        #RNN, LSTM\n",
        "        #LSTM : RNN의 상위 호환으로 RNN의 구조에 장/단기 기억을 가능하게 설계한 신경망 구조이다.        \n",
        "        with tf.variable_scope(\"lstm-head\") as scope:\n",
        "            #self.lstm_cell_head = tf.contrib.rnn.LSTMCell(embedding_size,state_is_tuple=True)\n",
        "            #self.lstm_out_head,self.lstm_state_head = tf.nn.dynamic_rnn(self.lstm_cell_head,self.embedded_chars_head,dtype=tf.float32)\n",
        "            #self.lstm_out_expanded_head = tf.expand_dims(self.lstm_out_head, -1)\n",
        "            self.lstm_out_head,self.lstm_state_head = bi_rnn(GRUCell(embedding_size), GRUCell(embedding_size), inputs = self.embedded_chars_head , dtype=tf.float32)\n",
        "            self.lstm_out_merge_head = tf.concat(self.lstm_out_head, axis=2)\n",
        "            #concat : 한 차원을 기준으로 텐서를 이어 붙임\n",
        "            self.Attention_head = tf.nn.softmax(\n",
        "                    tf.map_fn(lambda x: tf.matmul(self.W_s2, x), \n",
        "                              tf.tanh(\n",
        "                                      tf.map_fn(\n",
        "                                              lambda x: tf.matmul(self.W_s1, tf.transpose(x)),lstm_out_merge_head))))\n",
        "            \n",
        "            #self.lstm_out_head_fw = self.lstm_out_head[0]\n",
        "            #self.lstm_out_head_bw = self.lstm_out_head[1]\n",
        "            #self.lstm_out_merge_head = tf.concat([self.lstm_out_head_fw[-1], self.lstm_out_head_bw[-1]], axis=1)\n",
        "            self.lstm_out_expanded_head = tf.expand_dims(self.Attention_head, -1)\n",
        "            print(self.lstm_out_expanded_head.shape)\n",
        "\n",
        "#output = tf.stack(output, axis=1)\n",
        "#output = tf.reshape(output, [-1, FLAGS.num_units * 2])\n",
        "\n",
        "        with tf.variable_scope(\"lstm-body\") as scope:\n",
        "            #self.lstm_cell_body = tf.contrib.rnn.LSTMCell(embedding_size,state_is_tuple=True)\n",
        "            #self.lstm_out_body,self.lstm_state_body = tf.nn.dynamic_rnn(self.lstm_cell_body,self.embedded_chars_body,dtype=tf.float32)\n",
        "            #self.lstm_out_expanded_body = tf.expand_dims(self.lstm_out_body, -1)\n",
        "            self.lstm_out_body,self.lstm_state_body = bi_rnn(GRUCell(embedding_size), GRUCell(embedding_size), inputs = self.embedded_chars_body , dtype=tf.float32)\n",
        "            self.lstm_out_merge_body = tf.concat(self.lstm_out_body, axis=2)\n",
        "            #self.lstm_out_body_fw = self.lstm_out_body[0]\n",
        "            #self.lstm_out_body_bw = self.lstm_out_body[1]\n",
        "            #self.lstm_out_merge_body = tf.concat([self.lstm_out_body_fw[-1], self.lstm_out_body_bw[-1]], axis=1)\n",
        "            self.lstm_out_expanded_body = tf.expand_dims(self.lstm_out_merge_body, -1)\n",
        "            print(self.lstm_out_expanded_body.shape)\n",
        "        \n",
        "        self.pooled_outputs_head = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-head-%s\" % filter_size):\n",
        "                # CNN\n",
        "                filter_shape = [filter_size, embedding_size*2, 1, 256]\n",
        "                W_head = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W_head\")\n",
        "                b_head = tf.Variable(tf.constant(0.1, shape=[256]), name=\"b_head\")\n",
        "                conv_head = tf.nn.conv2d(\n",
        "                    self.lstm_out_expanded_head,\n",
        "                    W_head,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h_head = tf.nn.relu(tf.nn.bias_add(conv_head, b_head), name=\"relu_head\")\n",
        "                # Maxpooling over the outputs\n",
        "                pooled_head = tf.nn.max_pool(\n",
        "                    h_head,\n",
        "                    ksize=[1, sequence_length_head - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                self.pooled_outputs_head.append(pooled_head)\n",
        "\n",
        "        self.pooled_outputs_body = []\n",
        "        for i, filter_size in enumerate(filter_sizes):\n",
        "            with tf.name_scope(\"conv-maxpool-body-%s\" % filter_size):\n",
        "                # CNN\n",
        "                filter_shape = [filter_size, embedding_size*2, 1, 1024]\n",
        "                W_body = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W_body\")\n",
        "                b_body = tf.Variable(tf.constant(0.1, shape=[1024]), name=\"b_body\")\n",
        "                conv_body = tf.nn.conv2d(\n",
        "                    self.lstm_out_expanded_body,\n",
        "                    W_body,\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding=\"VALID\",\n",
        "                    name=\"conv\")\n",
        "                # Apply nonlinearity\n",
        "                h_body = tf.nn.relu(tf.nn.bias_add(conv_body, b_body), name=\"relu_body\")\n",
        "                #bias_add : bias+value\n",
        "                # Maxpooling over the outputs\n",
        "                pooled_body = tf.nn.max_pool(\n",
        "                    h_body,\n",
        "                    ksize=[1, sequence_length_body - filter_size + 1, 1, 1],\n",
        "                    strides=[1, 1, 1, 1],\n",
        "                    padding='VALID',\n",
        "                    name=\"pool\")\n",
        "                self.pooled_outputs_body.append(pooled_body)        \n",
        "        \n",
        "        l2_loss = tf.constant(0.0)\n",
        "        \n",
        "        pooled_outputs = tf.concat([self.pooled_outputs_head,self.pooled_outputs_body],-1,name='preconcat')\n",
        "        print(pooled_outputs.shape)\n",
        "        num_filters_total = num_filters * len(filter_sizes)\n",
        "        self.h_pool = tf.concat(pooled_outputs, 3, name='concat')\n",
        "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
        "      \n",
        "      \n",
        "        W_fc1 = tf.Variable(tf.truncated_normal([1280,1024],stddev=0.1),name=\"W_fc1\")\n",
        "        b_fc1 = tf.Variable(tf.constant(0.1,shape=[1024]),name=\"b_fc1\")\n",
        "        h_fc1 = tf.nn.relu(tf.matmul(self.h_pool_flat,W_fc1) + b_fc1)\n",
        "            \n",
        "        # Add dropout\n",
        "        with tf.name_scope(\"dropout\"):\n",
        "            self.h_drop = tf.nn.dropout(h_fc1, self.dropout_keep_prob)\n",
        "\n",
        "        # Final (unnormalized) scores and predictions\n",
        "        with tf.name_scope(\"output\"):\n",
        "            W = tf.get_variable(\n",
        "                \"W\",\n",
        "                shape=[1024, num_classes],\n",
        "                initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
        "            l2_loss += tf.nn.l2_loss(W)\n",
        "            l2_loss += tf.nn.l2_loss(b)\n",
        "            #l2_loss : output->sum(t**2)/2\n",
        "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
        "            #xw_plus_b : x*w+b\n",
        "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
        "\n",
        "        # CalculateMean cross-entropy loss\n",
        "        with tf.name_scope(\"loss\"):\n",
        "          #name_scope() 그래프에서 지정되는 대상이 연산자\n",
        "            print(self.scores.shape)\n",
        "            losses = tf.nn.softmax_cross_entropy_with_logits(logits = self.scores, labels = self.input_y)\n",
        "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
        "\n",
        "        # Accuracy\n",
        "        with tf.name_scope(\"accuracy\"):\n",
        "            print(\"%d/%d\",self.predictions,self.input_y)\n",
        "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
